# Transformer-Encoder-for-Tweets-Data-Encoding


Transformer-Encoder-for-Tweets-Data-Encoding is an implementation of just the encoder part of the Transformer.
This is based on Attention is all you need.

# What is covered?

In this, I have implemented the Transformer encoder. Used Word2Vec which is an NLP framework to convert words to numbers that are understood by the computer. Transformers have been very much useful to the current technologies and many breakthroughs have been made by creating GPT-3.

![image](https://raw.githubusercontent.com/bhanuprasanna527/transformer_encoder-for-Tweets-encoding/2a812997256b42ae2bc73cd092ea415b53dba25d//Transformer%20Encoder.png)

- We have used a Twitter Dataset present in Kaggle.
- Preprocessed the Dataset by removing emojis, punctuations, hashtags, special characters, and URLs.
- Used NLP techniques like Lemmatization, and Stemming.
- Implemented using PyTorch, Normalization techniques.

## License
[MIT](https://choosealicense.com/licenses/mit/)
